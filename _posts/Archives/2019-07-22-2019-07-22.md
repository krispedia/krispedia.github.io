---
layout: post
title: "[Daily] 2019.07.18"
categories: 
tag : []
---

## Deep Learning 

#### Gradient Vanishing
가중치의 편미분 결과로 가중치의 값이 0으로 수렴하는 경우<br>

#### Batch
한번의 학습 신경망에 들어가는 입력의 갯수<br>
전체 학습 데이터의 일부<br>

#### Batch Normalization 
Gradient Vanishing 이 생기지 않도록 가중치 값을 펼쳐주는 것<br>

#### Over Fitting
과적합이라 부르며 <br>
1.학습 파라미터가 너무 많은 경우<br>
2.학습 데이터가 부족한 경우<br>
생긴다. <br>

#### Drop Out
과적합이 되는 것을 방지하기 위해 학습 노드를 랜덤으로 선택하는 방법<br>

#### L1 Regularization

#### L2 Regularization

#### K차 교차검증

데이터 포인트가 많지 않은 경우 어떤 데이터 포인트가 선택 되었는지에 따라 검증 점수가 크게 달라진다. 이러한 점을 보완하기 위해 K개의 분할로 나누고 K-1개의 분할에서 훈련하고 나머지 분할에서 평가하는 방법이다. <br>
여기서 최종 점수는 각 분할(폴드)에서 나온 검증 점수의 평군이다. <br>

**Label Encoder** : <br>
글자를 숫자로 바꿔줘야 하는 경우 사용한다. 